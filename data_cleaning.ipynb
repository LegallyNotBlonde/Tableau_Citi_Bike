{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Could not infer format, so each element will be parsed individually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the file containing data stes\n",
    "path_feb = 'Resources/JC-202402-citibike-tripdata.csv'\n",
    "path_mar = 'Resources/JC-202403-citibike-tripdata.csv'\n",
    "path_apr = 'Resources/JC-202404-citibike-tripdata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files\n",
    "trip_data_feb = pd.read_csv(path_feb)\n",
    "trip_data_mar = pd.read_csv(path_mar)\n",
    "trip_data_apr = pd.read_csv(path_apr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns in February Data that do not match other months:\n",
      "set()\n",
      "\n",
      "Columns in March Data that do not match other months:\n",
      "set()\n",
      "\n",
      "Columns in April Data that do not match other months:\n",
      "set()\n",
      "If the sets are empty, it means all column names are identical.\n"
     ]
    }
   ],
   "source": [
    "# Verify if columns name are identical across all data sets\n",
    "\n",
    "# Get the column headers from each dataset\n",
    "columns_feb = set(trip_data_feb.columns)\n",
    "columns_mar = set(trip_data_mar.columns)\n",
    "columns_apr = set(trip_data_apr.columns)\n",
    "\n",
    "# Compare the columns across the datasets\n",
    "common_columns = columns_feb.intersection(columns_mar, columns_apr)\n",
    "feb_unique_columns = columns_feb - common_columns\n",
    "mar_unique_columns = columns_mar - common_columns\n",
    "apr_unique_columns = columns_apr - common_columns\n",
    "\n",
    "# print columns in our data sets that do not match\n",
    "print(\"\\nColumns in February Data that do not match other months:\")\n",
    "print(feb_unique_columns)\n",
    "\n",
    "print(\"\\nColumns in March Data that do not match other months:\")\n",
    "print(mar_unique_columns)\n",
    "\n",
    "print(\"\\nColumns in April Data that do not match other months:\")\n",
    "print(apr_unique_columns)\n",
    "print(\"If the sets are empty, it means all column names are identical.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the DataFrames\n",
    "combined_data = pd.concat([trip_data_feb, trip_data_mar, trip_data_apr])\n",
    "# Reset the index for the combined data\n",
    "combined_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride_id                object\n",
      "rideable_type          object\n",
      "started_at             object\n",
      "ended_at               object\n",
      "start_station_name     object\n",
      "start_station_id       object\n",
      "end_station_name       object\n",
      "end_station_id         object\n",
      "start_lat             float64\n",
      "start_lng             float64\n",
      "end_lat               float64\n",
      "end_lng               float64\n",
      "member_casual          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Show columns and their data types as a first step of cleaning the data\n",
    "print(combined_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to datetime format from currecnt 'object' type\n",
    "# The exact time is left as this information may be useful later\n",
    "combined_data['started_at'] = pd.to_datetime(combined_data['started_at'])\n",
    "combined_data['ended_at'] = pd.to_datetime(combined_data['ended_at'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started_at          datetime64[ns]\n",
      "ended_at            datetime64[ns]\n",
      "duration_minutes             int32\n",
      "dtype: object\n",
      "            ride_id  rideable_type          started_at            ended_at  \\\n",
      "0  A841EF9C3617E47B  electric_bike 2024-02-12 16:17:19 2024-02-12 16:23:24   \n",
      "1  514881A61CAD0A93  electric_bike 2024-02-04 15:04:16 2024-02-04 15:40:14   \n",
      "2  511F37086CCC4510  electric_bike 2024-02-16 16:23:16 2024-02-16 16:27:16   \n",
      "3  27A2AA8BDD0D1F6E   classic_bike 2024-02-25 17:05:01 2024-02-25 17:10:53   \n",
      "4  2808ABEC0903C18E  electric_bike 2024-02-29 15:21:15 2024-02-29 15:26:03   \n",
      "\n",
      "       start_station_name start_station_id            end_station_name  \\\n",
      "0         Adams St & 2 St            HB407  Stevens - River Ter & 6 St   \n",
      "1         Adams St & 2 St            HB407  Stevens - River Ter & 6 St   \n",
      "2  Clinton St & Newark St            HB409  Stevens - River Ter & 6 St   \n",
      "3         Adams St & 2 St            HB407  Stevens - River Ter & 6 St   \n",
      "4         Adams St & 2 St            HB407  Stevens - River Ter & 6 St   \n",
      "\n",
      "  end_station_id  start_lat  start_lng    end_lat    end_lng member_casual  \\\n",
      "0          HB602  40.739701 -74.036831  40.743133 -74.026989        casual   \n",
      "1          HB602  40.739741 -74.036824  40.743133 -74.026989        casual   \n",
      "2          HB602  40.737508 -74.035715  40.743133 -74.026989        casual   \n",
      "3          HB602  40.739814 -74.036904  40.743133 -74.026989        member   \n",
      "4          HB602  40.739712 -74.036858  40.743133 -74.026989        member   \n",
      "\n",
      "   duration_minutes  \n",
      "0                 6  \n",
      "1                35  \n",
      "2                 4  \n",
      "3                 5  \n",
      "4                 4  \n"
     ]
    }
   ],
   "source": [
    "# Create a new column showing the duration of each ride\n",
    "combined_data['duration_minutes'] = ((combined_data['ended_at'] - combined_data['started_at']).dt.total_seconds() / 60).astype(int)\n",
    "\n",
    "# Verify if datetime was changed correctly and duration is right\n",
    "print(combined_data.dtypes[['started_at','ended_at','duration_minutes']])\n",
    "print(combined_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in combined_data: 200310\n"
     ]
    }
   ],
   "source": [
    "# Display the number of rows in combined_data\n",
    "number_rows = len(combined_data)\n",
    "print(f\"Number of rows in combined_data: {number_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "duplicates = combined_data.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All start station IDs are consistently associated with a single start station name.\n",
      "All end station IDs are consistently associated with a single start station name.\n"
     ]
    }
   ],
   "source": [
    "# Check potential misspells in the station names:\n",
    "\n",
    "# Group the data by 'station_id' and 'station_name'\n",
    "# Then check if each 'station_id' is associated with more than one 'station_name'\n",
    "\n",
    "# Group by 'start_station_id' and count the number of unique 'start_station_name' for each ID\n",
    "start_station_consistency = combined_data.groupby('start_station_id')['start_station_name'].nunique()\n",
    "end_station_consistency = combined_data.groupby('end_station_id')['end_station_name'].nunique()\n",
    "\n",
    "# Filter for station IDs that have more than one associated station name\n",
    "inconsistent_start_stations = start_station_consistency[start_station_consistency > 1]\n",
    "inconsistent_end_stations = end_station_consistency[end_station_consistency > 1]\n",
    "\n",
    "# To see the specific rows with inconsistencies for start stations\n",
    "if not inconsistent_start_stations.empty:\n",
    "    inconsistent_rows = combined_data[combined_data['start_station_id'].isin(inconsistent_start_stations.index)]\n",
    "    print(\"\\nRows with inconsistent start station names for the same station ID:\")\n",
    "    print(inconsistent_rows)\n",
    "else:\n",
    "    print(\"All start station IDs are consistently associated with a single start station name.\")\n",
    "# To see the specific rows with inconsistencies for start stations\n",
    "if not inconsistent_end_stations.empty:\n",
    "    inconsistent_end_rows = combined_data[combined_data['start_station_id'].isin(inconsistent_end_stations.index)]\n",
    "    print(\"\\nRows with inconsistent start station names for the same station ID:\")\n",
    "    print(inconsistent_end_rows)\n",
    "else:\n",
    "    print(\"All end station IDs are consistently associated with a single start station name.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inconsistent start stations: 0\n",
      "Inconsistent end stations: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if station IDs match with station names \n",
    "inconsistent_start_stations = combined_data[combined_data['start_station_name'].isna() & combined_data['start_station_id'].notna()]\n",
    "inconsistent_end_stations = combined_data[combined_data['end_station_name'].isna() & combined_data['end_station_id'].notna()]\n",
    "\n",
    "print(f\"Inconsistent start stations: {len(inconsistent_start_stations)}\")\n",
    "print(f\"Inconsistent end stations: {len(inconsistent_end_stations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid start coordinates: 0\n",
      "Invalid end coordinates: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for invalid latitude and longitude values\n",
    "invalid_start_coords = combined_data[(combined_data['start_lat'] < -90) | (combined_data['start_lat'] > 90) |\n",
    "                                     (combined_data['start_lng'] < -180) | (combined_data['start_lng'] > 180)]\n",
    "invalid_end_coords = combined_data[(combined_data['end_lat'] < -90) | (combined_data['end_lat'] > 90) |\n",
    "                                   (combined_data['end_lng'] < -180) | (combined_data['end_lng'] > 180)]\n",
    "\n",
    "print(f\"Invalid start coordinates: {len(invalid_start_coords)}\")\n",
    "print(f\"Invalid end coordinates: {len(invalid_end_coords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rides that end before they start: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if any rides end before they start (incorrect date records)\n",
    "invalid_dates = combined_data[combined_data['ended_at'] < combined_data['started_at']]\n",
    "print(f\"Rides that end before they start: {len(invalid_dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [ride_id, rideable_type, started_at, ended_at, start_station_name, start_station_id, end_station_name, end_station_id, start_lat, start_lng, end_lat, end_lng, member_casual, duration_minutes]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check if any other mispelling in the data in 'casual_member' column\n",
    "invalid_member_casual = combined_data[~combined_data['member_casual'].isin(['casual', 'member'])]\n",
    "# Display the rows with invalid entries\n",
    "print(invalid_member_casual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_station_name     99\n",
      "start_station_id       99\n",
      "end_station_name      554\n",
      "end_station_id        583\n",
      "end_lat                54\n",
      "end_lng                54\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if the data is missing any values\n",
    "missing_values = combined_data.isnull().sum()\n",
    "# Show the columns with missing values\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with any missing values\n",
    "combined_data_cleaned = combined_data.dropna()\n",
    "# Ensure the data no longer has any missing values\n",
    "missing_data = combined_data_cleaned.isnull().sum()\n",
    "# Show the columns with missing values\n",
    "print(missing_data[missing_data > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in combined_data: 199674\n"
     ]
    }
   ],
   "source": [
    "# Display the number of rows in combined_data_cleaned\n",
    "number_rows_clean = len(combined_data_cleaned)\n",
    "print(f\"Number of rows in combined_data: {number_rows_clean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ride_id  rideable_type          started_at            ended_at  \\\n",
      "0  A841EF9C3617E47B  electric_bike 2024-02-12 16:17:19 2024-02-12 16:23:24   \n",
      "1  514881A61CAD0A93  electric_bike 2024-02-04 15:04:16 2024-02-04 15:40:14   \n",
      "2  511F37086CCC4510  electric_bike 2024-02-16 16:23:16 2024-02-16 16:27:16   \n",
      "3  27A2AA8BDD0D1F6E   classic_bike 2024-02-25 17:05:01 2024-02-25 17:10:53   \n",
      "4  2808ABEC0903C18E  electric_bike 2024-02-29 15:21:15 2024-02-29 15:26:03   \n",
      "\n",
      "       start_station_name start_station_id            end_station_name  \\\n",
      "0         Adams St & 2 St            HB407  Stevens - River Ter & 6 St   \n",
      "1         Adams St & 2 St            HB407  Stevens - River Ter & 6 St   \n",
      "2  Clinton St & Newark St            HB409  Stevens - River Ter & 6 St   \n",
      "3         Adams St & 2 St            HB407  Stevens - River Ter & 6 St   \n",
      "4         Adams St & 2 St            HB407  Stevens - River Ter & 6 St   \n",
      "\n",
      "  end_station_id  start_lat  start_lng    end_lat    end_lng member_casual  \\\n",
      "0          HB602  40.739701 -74.036831  40.743133 -74.026989        casual   \n",
      "1          HB602  40.739741 -74.036824  40.743133 -74.026989        casual   \n",
      "2          HB602  40.737508 -74.035715  40.743133 -74.026989        casual   \n",
      "3          HB602  40.739814 -74.036904  40.743133 -74.026989        member   \n",
      "4          HB602  40.739712 -74.036858  40.743133 -74.026989        member   \n",
      "\n",
      "   duration_minutes  \n",
      "0                 6  \n",
      "1                35  \n",
      "2                 4  \n",
      "3                 5  \n",
      "4                 4  \n"
     ]
    }
   ],
   "source": [
    "# Display first 5 rows in our data\n",
    "print(combined_data_cleaned.head(5))\n",
    "# Save cleaned data to csv file\n",
    "combined_data_cleaned.to_csv('Output_Files/tripdata_feb_to_apr_2024.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
